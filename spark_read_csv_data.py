# -*- coding: utf-8 -*-
"""spark_read_csv data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rPTiOP7ebDTZdEfV_0mzBSDvew2q_mpM
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, col
# Initialize Spark session
spark = SparkSession.builder.appName("SparkExample").getOrCreate()

# Load data into a DataFrame
df = spark.read.csv("california_housing_train.csv", header=True, inferSchema=True)

# Show the first few rows
df.show()

# Perform a transformation: Select specific columns
df_filtered = df.select("population", "total_rooms")

# Perform an action: Count the number of rows
count = df_filtered.count()


#result = df_filtered.agg(sum("total_rooms")).collect()[0][0]
result = df_filtered.agg(sum(col("population").cast("double"))).collect()[0][0]
print(result)
print(f"Number of rows: {count}")

# Stop the Spark session
spark.stop()

from pyspark.sql.functions import sum, col
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("SparkExample").getOrCreate()
products_data = [
    (1, "Product A", "Category 1"),
    (2, "Product B", "Category 1"),
    (3, "Product C", "Category 2"),
    (4, "Product D", "Category 2"),
    (5, "Product E", "Category 3")
]

product_dataframe_columns = ["product_id", "product_name", "category"]

sales_data = [
    (1001, 1, 10, 15.0, "2023-1-1"),
    (1002, 2, 5, 20.0, "2023-1-2"),
    (1003, 3, 7, 25.0, "2023-1-3"),
    (1004, 4, 12, 30.0, "2023-1-4"),
    (1005, 5, 3, 35.0, "2023-1-5"),
    (1006, 1, 8, 15.0, "2023-2-1"),
    (1007, 2, 10, 20.0, "2023-2-2"),
    (1008, 3, 2, 25.0, "2023-2-2"),
    (1009, 4, 6, 30.0, "2023-2-4"),
    (1010, 5, 14, 35.0, "2023-2-5"),
    # Anomalies
    (1011, 1, 50, 50.0, "2023-2-2"),
    (1012, 2, 60, 60.0, "2023-3-2"),
    (1013, 3, 70, 70.0, "2023-3-3"),
    (1014, 4, 80, 80.0, "2023-1-1"),
    (1015, 5, 90, 1.0, "2023-2-2"),
]

sales_columns = ["transaction_id", "product_id", "quantity", "price", "date"]

def create_product_dataframe(spark):
  return spark.createDataFrame(products_data, product_dataframe_columns)

def create_sales_dataframe(spark):
  return spark.createDataFrame(sales_data, sales_columns)

def df(spark):
   return spark.createDataFrame()


product_dataframe = create_product_dataframe(spark)
sales_dataframe = create_sales_dataframe(spark)

product_dataframe.show()
sales_dataframe.show()
print('start')

joined_df = sales_dataframe.join(product_dataframe, on="product_id", how="inner")
category1_df = joined_df.filter(joined_df.category == "Category 1")
category1_sales = category1_df.withColumn("total_sales", col("quantity") * col("price"))
total_sales_category1 = category1_sales.agg(sum("total_sales").alias("total_sales_amount"))
total_sales_category1.show()

spark.stop()